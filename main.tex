\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[english]{babel}
\usepackage{gensymb}

\begin{document}
	\title{Linear Algebra Summary}
	\author{Sven Thiessen}
	
	\maketitle
	\newpage
	
	\tableofcontents
	\newpage
	
	\section{Definitions}
		\subsection{Complex Numbers}
			\subsubsection{Absolute Value and Argument}
			The absolute value of a complex number is the distance to the origin. The absolute value of a complex number $z=a+bi$ is denoted by $|z|$, and equals $\sqrt{a^2+b^2}$.
			
			The argument is the angle the directed segment from the origin to the complex numbers makes with the positive real axis. The argument is only defined for nonzero complex numbers.
			
			From the absolute value and argument it is straightforward to find it's real and imaginary parts:
			\begin{eqnarray*}
				Re(z) &=& |z|\cos{(\arg{z})} \\
				Im(z) &=& |z|\sin{(\arg{z})}
			\end{eqnarray*}
			\subsubsection{Real part and imaginary part}
			If $z = a+bi$ with $a,b \in \mathbb{R}$, then $a$ is called the real part of $z$ denoted by $Re(z)$, and b is called the imaginary part of z, denoted by $Im(z)$.
			
			For instance for all $z_1,z_2$ we have:
			\begin{eqnarray*}
				Re(z_1+z_2) &=& Re(z_1) + Re(z_2) \\
				Im(z_1+z_2) &=& Im(z_1) + Im(z_1) \\
				Re(z_1z_2) &=& Re(z_1)Re(z_2) - Im(z_1)Im(z_2) \\
				Im(z_1z_2) &=& Re(z_1)Im(z_2) + Im(z_1)Re(z_2) \\
			\end{eqnarray*}
		
			\subsubsection{Triangle inequality}
			\begin{equation*}
				|z + w| \leq |z| + |w|
			\end{equation*}
			
			\subsubsection{The complex conjugate}
			If $z = a+bi$ with $a,b \in \mathbb{R}$ is a complex number, then $a-bi$ is called the complex conjugate of $z$, denoted by $\overline{z}$. Geometrically speaking, conjugation is a reflection in the real axis.
			
			\subsubsection{Complex exponential function}
			For very complex number $z$ we define the complex number $e^z$ by giving its absolute value and argument:
			\begin{eqnarray*}
				|e^z| &=& e^{Re(z)}, \\
				\arg{e^z} &=& Im(z)
			\end{eqnarray*}
		
			\subsubsection{Complex sine and cosine}
			\begin{eqnarray*}
				\cos{z} &=& \frac{1}{2}(e^{iz} + e^{-iz}), \\
				\sin{z} &=& \frac{1}{2i}e^{iz} - e^{-iz})
			\end{eqnarray*}
		
			\subsubsection{Complex polynomial}
			An expression in the form
			\begin{equation*}
				a_nz^n + a{n-1}z^{n-1} + \dots + a_1z+a_0
			\end{equation*}
			
			in which $a_0,\dots,a_n$ are complex numbers, is called a complex polynomial. Let $p(z)$ be a polynomial. If $p(\alpha) = 0$, then $\alpha$ is called a zero or root of the polynomial.
			
			\subsubsection{Lines and segments of the (complex) plane}
			If $z \neq 0$ is a complex number, then the numbers $tz$ with, $t \in \mathbb{R}$, describe the line through $0$ and $z$. The segment with the endpoints $0$ and $z$ is described by taking $t$ in the interval $[0,1]$. We sometimes denote this segment by $[0,z]$. The midpoint of the segment $[0,z]$ is the complex number $\frac{1}{2}z$. For $t = \frac{1}{2}$. we find the midpoint of $[z,w]$:
			\begin{equation*}
				w + \frac{1}{2}(z-w) \text{or} \frac{1}{2}(z+w)
			\end{equation*}
			
			The length of the segment $[z,w]$ is equal to the distance between $z$ and $w$, i.e., $|w-z|$.
			
			The lines through $z_1$ and $z_2$ (with $z_1 \neq z_2$) and through $w_1$ and $w_2$ with ($w_1 \neq w_2)$, respectively, are parallel if and only if $w_2 - w_1$ is a real multiple of $z_2-z_1$, or, equivalently, the quotient $\frac{w_2-w_1}{z_2-z_1} \in \mathbb{R}$
			
			\subsubsection{Translations}
			Let $u$ be a complex number. The map $T: \mathbb{C} \to \mathbb{C}$ given by $T(z) = z+u$ is a translation over $u$. Translations 'preserve shapes', so, for example, they transform straight lines into straight lines.

		\subsection{Vectors in two and three dimensions}
			\subsubsection{Linear combinations}
			If $\vec{v_1},\vec{v_2},,..,\vec{v_k}$ are $k$ vectors and $\lambda_1,\dots,\lambda_k$ are $k$ real numbers (scalars), then the vector:
			\begin{equation*}
				\lambda_1\vec{v_1} + \lambda_2+\vec{v_2} + \dots + \lambda_k+\vec{v_k}
			\end{equation*}
			is called a linear combination of these vectors.
			
			\subsubsection{Length}
			The length of $\vec{x}$ is denoted by $||x||$. The distance between the two vectors $\vec{u}$ and $\vec{v}$ is by defenition the length of the difference $\vec{u}- \vec{v}$ (or $\vec{v} - \vec{u}$), so $||\vec{u} - \vec{v}||$.
			
			\subsubsection{The inner product}
			The inner product of two vectors $\vec{u}$ and $\vec{v}$, both $\vec{0}$, is defined as
			\begin{equation*}
				||\vec{u}|| \cdot ||\vec{v}|| \cdot \cos{\rho}.
			\end{equation*}
			The inner product is denoted by $(\vec{u}, \vec{v})$.
			
		\subsubsection{Orthogonality}
		If two non-zero vector have inner product 0, then they are perpendicular (the angle between them is $\pm 90\degree$ or $\pm\frac{\pi}{2})$ since the cosine of the angle between them is 0. conversely, if two non-zero vectors are perpendicular then their inner product is 0. Now the zero vector has inner product 0 with any vector, and we agree to say that the zero vector is perpendicular to any vector. This is a convenient convention since then we have: The inner product of two vectors is 0 if and only if the vectors are perpendicular.
		
		\subsubsection{Cross product}
		The cross product of $\vec{v}\times\vec{u}$ of the vectors $\vec{v} = (v_1, v_2, v_3)$ and $\vec{w} = (w_1, w_2, w_3)$ is by definition the vector
		\begin{equation*}
			(v_2w_3 - v_3w_2, v_3w_1 - v_1w_3, v_1w_2 - v_2w_1).
		\end{equation*}
		The cross product always gives a vector perpendicular to $\vec{u}$ and $\vec{w}$.
		
		\subsection{Matrices and systems of linear equations}
			\subsubsection{Matrix}
			A matrix is a rectangular array of numbers or elements from some arithmetical structure.
			
			\subsubsection{The opposite matrix}
			The matrix $B = (-1)A$ satisfies $A+B=0$ and is called the $(additive) opposite of A$. Instead of $(-1)A$ we usually write $-A$.
			
			\subsubsection{The identity matrix}
			The $n\times n$-matrix.
			\begin{equation*}
				I = \begin{pmatrix}
					1 && 0 && \dots && 0 \\
					0 && 1 && \ddots && \vdots\\
					\vdots && \ddots && \ddots && 0 \\
					0 && \dots && 0 && 1
				\end{pmatrix}
			\end{equation*}
			
			\subsubsection{The (multiplicative) inverse of a matrix}
			If, given a $n\times n$-matrix A, there exists a matrix $n\times n$ B with $AB = BA = I$ then B is called the inverse of A. We usually denote this inverse by $A^{-1}$. Keep in mind that there also exist non-zero matrices without an inverse.
			
			\subsubsection{Inverse of B and A}
			Let $A$ and $B$ be $n \times n$-matrices and suppose that $A^{-1}$ and $B^{-1}$ exist. Then 
			\begin{equation*}
				(AB)^{-1} = B^{-1}A^{-1}
			\end{equation*}
			
			\subsubsection{Transpose of a matrix}
			If $A = (a_{ij})$ is a $n \times m$-matrix, then its transpose $A^T$ is the $m \times n$ matrix, whose $i$-th row equals the $i$-th column of A (for $i$ = 1, \dots, m), so the $i,j$-th entry of $A^T$ equals $a_{ji}$. The $j$-th column is then automatically equal to the $j$-th row of A.
			
		\subsection{Vector spaces}
			\subsubsection{The notation of a vector space}
			For all vectors $\vec{p}, \vec{q}, \vec{r}$ and all scalars(numbers) $\lambda, \mu$ we have.
			\begin{enumerate}
				\item $\vec{p} + \vec{q} = \vec{q} + \vec{p}$
				\item $(\vec{p}+\vec{q}) + \vec{r} = \vec{p} + (\vec{q} + \vec{r})$
				\item there is a zero vector $\vec{0}$ with the property $\vec{p}+\vec{0} = \vec{p}$ for every $\vec{p}$
				\item Every vector $\vec{p}$ has an opposite $-\vec{p}$ such that $\vec{p} + -\vec{p} = \vec{0}$ (we also write $\vec{p} - \vec{p}= \vec{0}$)
				\item $1\vec{p} = \vec{p}$
				\item $(\lambda\mu)\vec{p} = \lambda(\mu\vec{p})$
				\item $(\lambda + \mu)\vec{p} = \lambda\vec{p} + \mu\vec{p}$
				\item $\lambda(\vec{p} + \vec{q} = \lambda\vec{p} + \lambda\vec{q})$
			\end{enumerate}
			\subsubsection{Linear subspace}
			A non-empty subset $W$ of a vector space $V$ is called a linear subspace of $V$ if for all $\vec{p},\vec{q} \in W$ and for all $\lambda$ we have
			\begin{eqnarray*}
				\vec{p} + \vec{q} &\in& W\\
				\lambda\vec{p} &\in& W
			\end{eqnarray*}
		
			Equivalently: for all $\vec{p},\vec{q} \in W$ and for all scalars $\lambda, \mu$
			\begin{equation*}
				\lambda\vec{p} + \mu\vec{q} \in W
			\end{equation*}
			
			\subsubsection{Line}
			Let $\vec{p}$ and $\vec{v}$ be two vectors in a vector space and suppose $\vec{v} \neq \vec{0}$. Then the set of vectors for the form
			\begin{equation*}
				\vec{x} = \vec{p} + \lambda\vec{v}, \lambda \in \mathbb{R} \text{ or} \in \mathbb{C}
			\end{equation*}
			is called a line in the vector space. The vector $\vec{p}$ is called a position vector of the line and the vector $\vec{v}$ a direction vector. We call the description $\vec{x} = \vec{p} + \lambda\vec{v}$ a parametric equation or parametric representation of the line.
			
			\subsubsection{Plane}
			Let $\vec{p}, \vec{v}, \vec{q}$ be three vectors in a vector space and suppose $\vec{v} \neq \vec{0}, \vec{w} \neq \vec{0},$ and $\vec{v}$ and $\vec{w}$ are not multiples of one another. The set of vectors
			\begin{equation*}
				\vec{x} = \vec{p} + \lambda\vec{v}+ \mu\vec{w},\text{ } \lambda, \mu \in \mathbb{R} (\text{or }\mathbb{C})
			\end{equation*}
			is called a plane in the vector space with position vector $\vec{p}$ and direction vectors $\vec{v}$ and $\vec{w}$. The description is called a parametric equation of the plane.
			
			\subsubsection{Linear combination}
			Let $\vec{a}_1,\dots,\vec{a}_n$ be vectors in a vector space. The vector $\vec{x}$ is called a linear combination of $\vec{a}_1,\dots,\vec{a}_n$ if there exists scalars $\lambda_1,\dots,\lambda_n$ such that
			\begin{equation*}
				\vec{x} = \lambda_1\vec{a}_1 + \dots + \lambda_n\vec{a}_n
			\end{equation*}
			In this situation we also say that the vector $\vec{x}$ depends (or is linearly dependent) on the vectors $\vec{a}_1,\dots,\vec{a}_n$
			
			\subsubsection{Span of vectors}
			Let $\vec{a}_1,\dots,\vec{a}_n$ be vectors in a vector space. The set of all linear combinations $\vec{a}_1,\dots,\vec{a}_n$ is called the span of the vectors $\vec{a}_1,\dots,\vec{a}_n$ and is denoted as $<\vec{a}_1,\dots,\vec{a}_n>$.
			
			\subsubsection{Linearly in(dependent) set of vectors}
			A set or system of vectors $\vec{a}_1,\dots,\vec{a}_n$ is called linearly dependent if at east one of the vectors is a linear combination of the others. The vectors are called linearly independent if none of the vectors is a linear combination of the others. We often say in such situations that the vectors $\vec{a}_1,\dots,\vec{a}_n$ are linearly (in)dependent, or that the set $\{\vec{a}_1,\dots,\vec{a_n}\}$ is linearly independent.
			
			\subsubsection{Basis and dimension}
			A linearly independent set spanning a vector space $V$ is called a basis of $V$. The number of elements in the basis is called the dimension of $V$ is denoted as $\dim(V)$
			
			\subsubsection{Coordinates}
			Let $\vec{a}_1,\dots,\vec{a}_n$ be a basis of the vector space $V$. If 
			\begin{equation*}
				\vec{x} = x_1\vec{a}_1 + \dots + x_n\vec{a}_n
			\end{equation*}
			then the coefficients $x_1,\dots,x_n$ are called the coordinates of the vector $\vec{x}$ with respect to this basis. The vector $(x_1,\dots,x_n)$ is called the coordinate vector of $\vec{x}$ and it itself a vector in $\mathbb{R}^n$ or $\mathbb{C}^n$
			
		\subsubsection{Rank and inverse of a matrix, determinants}
			\subsubsection{Row and column space}
			Let $A$ be a matrix with n rows and m columns. Then every row has m entries so that these rows can be seen as vectors in $\mathbb{R}^m$ or $\mathbb{C}^m$; the subspace spanned by the rows is called the row space of the matrix. Similarly, every column is an element of $\mathbb{R}^n$ or $\mathbb{C}^n$; the space spanned by the columns is called the column space of the matrix.
			
			\subsubsection{Rank}
			The rank of a matrix is by definition the dimension of its row or cokumn space. Notation: rank(A).
			
			\subsubsection{$2 \times 2$-determinant}
			The $2\times 2$-determinant of a $2\times2$-matrix
			\begin{equation*}
				A = \begin{pmatrix}
					a & b \\
					c & d
				\end{pmatrix}
			\end{equation*}
			is the number $ad-bd$. Notation $det(A)$ and
			\[
				\left |
					\begin{matrix}
						a & b \\
						c & d
					\end{matrix}
				\right |.
			\]
			
			\subsubsection{Determinant function}
			A determinant function on $\mathbb{R}^n$ or $\mathbb{C}^n$ is a function D that assigns to every $n-tuple$ of vectors $\vec{a}_1,\dots,\vec{a}_n$ a real (or complex) number and hat the properties:
			\begin{enumerate}
				\item Multi-linearity: D is linear in every entry
				\item Antisymmetry by interchanging two vectors the determinant function changes sign.
				\item Normalization $D(\vec{e}_1,\dot,\vec{e}_n) = 1$.
			\end{enumerate}
		
		\subsection{Inner product}
		Let $V$ be a real vector space. An inner product on $V$ is a function which assigns to any two vectors $\vec{a},\vec{b}$ from $V$ a real number denoted by $(\vec{a},\vec{b})$ in such a way that
		\begin{enumerate}
			\item $(\vec{a}, \vec{b})$ is linear in both entries:
			\begin{eqnarray*}
				(\lambda\vec{v} + \mu\vec{w}, \vec{b}) &=& \lambda(\vec{v}, \vec{b}) + \mu(\vec{w}, \vec{b})\\
				(\vec{a}, \lambda\vec{v}+\mu\vec{w}) &=& \lambda(\vec{a}, \vec{v}) + \mu(\vec{a}, \vec{w})				
			\end{eqnarray*}
			for all scalars and all vectors.
			\item $(\vec{a}, \vec{b}) = (\vec{b}, \vec{a})$ for all $\vec{a},\vec{b} \in V$. (So we need only impose linearity in the first entry in the previous item, because symmetry will then automatically imply linearity in the second entry).
			\item $(\vec{a}, \vec{a}) \ge 0$ for all $\vec{a}\in V$ and $(\vec{a}, \vec{a}) = 0$ implies $\vec{a} = \vec{0}$.
		\end{enumerate}

		\subsubsection{Inner product complex vector space}
		Let V be a complex vector space. An inner product on V is a function that assigns to every two vectors $\vec{a}, \vec{b}$ in V a complex number, denoted by $(\vec{a}, \vec{b})$, in such a way that
		\begin{enumerate}
			\item $(\vec{a}, \vec{b})$ is linear in $(\vec{a})$
			\item $(\vec{a},\vec{b}) = \overline{(\vec{b}, \vec{a})}$ for all $\vec{a},\vec{b} \in V$
			\item $(\vec{a}, \vec{a}) \ge 0$ for all $\vec{a} \in V$, and $(\vec{a},\vec{a})=0$ implies $\vec{a} = \vec{0}$
		\end{enumerate}
		A complex vector space with an inner product is often called a (complex) inner product space.
			
		\subsubsection{Length and distance}
		In an inner product space the length or norm $||\vec{a}||$ of a vector $\vec{a}$ is defined as 
		\begin{equation*}
			||\vec{a}|| = \sqrt{(\vec{a}, \vec{a})}
		\end{equation*}
		The distance between the vectors $\vec{a}$ and $\vec{b}$ is by definition the length of the vector $\vec{a} - \vec{b}$, i.e. $||\vec{a}-\vec{b}||$.
		
		\subsubsection{Perpendicular vectors}
		Let V be an inner product space. The vectors $\vec{a} \ in V$ and $\vec{b} \in V$ are perpendicular, denoted by $\vec{a} \perp \vec{b}$, if $(\vec{a}, \vec{b} = 0)$. (In this definition $\vec{a}$ and/or $\vec{b}$ are allowed to be the zero vector; also denote that $(\vec{a}, \vec{b}) = 0 \iff (\vec{b}, \vec{a}) = 0)$).
		
		\subsubsection{Orthogonal complement}
		Let V be a real or complex inner product space and let W be a linear subspace of V. The set of vectors which are perpendicular to all vectors of W is denoted by $W^\bot$. More precisely:
		
		Let W be a linear subspace of an inner product space V. The orthogonal complement of W is the set
		\begin{equation*}
			W^\bot = \{\vec{x}\in V | (\vec{x},\vec{w}) = 0 \quad \forall \vec{w} \in W\}.
		\end{equation*}
		
		\subsubsection{Orthonormal sets of vectors}
		Sets of length 1 vectors which are mutually orthogonal are of special importance. Such sets of vectors are called orthonormal sets. An example is the standard basis $\vec{e}_1,\dots\vec{e}_n$ of $\mathbb{R}^n$ or $\mathbb{C}^n$. Orthonormal sets are useful in computing orthogonal projections, distances, and coordinates.
		
		Let V be an inner product space. The vectors $\vec{e}_1,\dots,\vec{e}_n$ in V form an orthonormal set if for $1 \leq i$, $i\leq n$
		  \[
				(\vec{e}_i,\vec{e}_j)=\left\{
					\begin{array}{ll}
						0 \text{ if } i \neq j \\
						1 \text{ if } i = j 
					\end{array}
				\right.
		\]
		If moreover $\vec{e}_1,\dots,\vec{e}_n$ is a basis of V, then the set is called an orthonormal basis of V.
\end{document}